{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Your full name\"  # write YOUR NAME\n",
    "\n",
    "honorPledge = \"I affirm that I have not given or received any unauthorized\\n\"+\\\n",
    "              \"help on this assignment, and that this work is my own.\\n\"\n",
    "\n",
    "\n",
    "print(\"\\nName: \", name)\n",
    "print(\"\\nHonor pledge: \", honorPledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:#FFFFFF;background-color:#2e4053;\">MAC0460 / MAC5832 (2022)</font>\n",
    "<hr>\n",
    "\n",
    "## <font style=\"background-color:#abebc6\">EP3: Logistic regression</font>\n",
    "\n",
    "### Topics / concepts explored in this EP:\n",
    "\n",
    "- Implementation of the **logistic regression algorithm**, using the gradient descent technique\n",
    "- Application on binary classification of toy datasets\n",
    "- Effects of parameters, unbalanced classes\n",
    "\n",
    "### Evaluation  criteria\n",
    "- Correctitude of the algorithms\n",
    "- Code\n",
    "    - do not change the prototype of the functions\n",
    "    - efficiency (you should avoid unnecessary loops; use matrix/vector computation with NumPy wherever appropriate)\n",
    "    - cleanliness (do not leave any commented code or useless variables)\n",
    "- Appropriateness of the answers\n",
    "- File format: Complete and submit this notebook <font color=\"red\">with the outputs of the execution</font>. **Do no change the file name.**<br>Places to be filled are indicated with <font style=\"background-color: #f7dc6f\">this color</font>\n",
    "\n",
    "### Remarks\n",
    "- In this notebook vectors are implemented as <tt>ndarray (n,)</tt> and not as <tt>ndarray (n,1)</tt>\n",
    "- It might be wise to first make sure your implementation is correct. For instance, you can compare the results of your implementation with the ones obtained with the implementation available in <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm\n",
    "\n",
    "We will use the formulation described in the textbook (<i>Learning from data</i>, Abu-Mostafa <i>et al.</i>).<br> <u>Note</u>: Positive class label is equal to +1 and negative class label is equal to -1.\n",
    "\n",
    "The loss (or cost) function to be minimized is\n",
    "$$\n",
    "E_{in}(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ln(1 + e^{-y^{(i)} \\mathbf{w}^T \\mathbf{x}^{(i)}}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Its gradient is given by\n",
    "\n",
    "$$\\nabla E_{in}(\\mathbf{w}) = - \\frac{1}{N}\\sum_{i=1}^{N} \\frac{y^{(i)} \\mathbf{x}^{(i)}}{1 + e^{y^{(i)} \\mathbf{w}^T \\mathbf{x}^{(i)}}}  \\tag{2}$$\n",
    "\n",
    "The logistic (sigmoid) function is\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}  \\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"background-color:#abebc6\">1. Training and prediction algorithms</font>\n",
    "\n",
    "<br>\n",
    "In the next four code cells, write the code to implement the specified functions. These functions should be used afterwards for some training and prediction cases. Use vectorial computation with NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color: #f7dc6f\">1.1. Cross-entropy loss</font>\n",
    "This is Equation (1) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(w, X, y):\n",
    "    \"\"\"\n",
    "    Computes the loss (equation 1)\n",
    "    :param w: weight vector\n",
    "    :type: np.ndarray(shape=(1+d, ))\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, 1+d))\n",
    "    :param y: class labels\n",
    "    :type y: np.ndarray(shape=(N, ))\n",
    "    :return loss: loss (equation 1)\n",
    "    :rtype: float\n",
    "    \"\"\"    \n",
    "    \n",
    "    ### ===> Your code begins here\n",
    "    raise NotImplementedError(\"Function cross_entropy_loss() is not implemented\")\n",
    "    ### ===> Your code ends here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color: #f7dc6f\">1.2. Gradient of the cross-entropy loss</font>\n",
    "\n",
    "This is Equation (2) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_gradient(w, X, y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the loss function (equation 2)\n",
    "    :param w: weight vector\n",
    "    :type: np.ndarray(shape=(1+d, ))\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, 1+d))\n",
    "    :param y: class labels\n",
    "    :type y: np.ndarray(shape=(N, ))\n",
    "    :return grad: gradient (equation 2)\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    \n",
    "    ### ===> Your code begins here \n",
    "    raise NotImplementedError(\"Function cross_entropy_gradient() is not implemented\")\n",
    "    ### ===> Your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color: #f7dc6f\">1.3 Logistic regression training</font>\n",
    "\n",
    "The function below receives the data matrix  <tt> X (shape = (N, d))</tt> and the ouput vector <tt>y (shape = (N,))</tt>, and should return the final weight vector <tt>w (shape = (d+1,))</tt> and, optionally (when  parameter <tt>return_history = True</tt>), a list of size <tt>num_iterations+1</tt> with the cross-entropy loss values at the beginning and after each of the iterations.\n",
    "\n",
    "Note that the data matrix needs to be extended with a column of 1's.\n",
    "\n",
    "If <tt>w0==None</tt>, then it must be initialized  with <tt>w0 = np.random.normal(loc = 0, scale = 1, size = X.shape[1])</tt>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic(X, y, learning_rate = 1e-1, w0 = None,\\\n",
    "                        num_iterations = 300, return_history = False):\n",
    "    \"\"\"\n",
    "    Computes the weight vector applying the gradient descent technique\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: class label\n",
    "    :type y: np.ndarray(shape=(N, ))\n",
    "    :return: weight vector\n",
    "    :rtype: np.ndarray(shape=(1+d, ))\n",
    "    :return: the history of loss values (optional)\n",
    "    :rtype: list of float\n",
    "    \"\"\"    \n",
    "     \n",
    "    ### ===> Your code begins here\n",
    "    raise NotImplementedError(\"Function train_logistic() is not implemented\")\n",
    "    ### ===> Your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color: #f7dc6f\">1.4. Logistic regression prediction</font>\n",
    "\n",
    "The function in the next cell will be used to do the prediction. Recall that the prediction is a score in $[0,1]$, given by the sigmoid value of the linear combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def predict_logistic(X, w):\n",
    "    \"\"\"\n",
    "    Computes the logistic regression prediction\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N,d))\n",
    "    :param w: weight vector\n",
    "    :rtype: np.ndarray(shape=(1+d,))\n",
    "    :return: predicted classes \n",
    "    :rtype: np.ndarray(shape=(N,))\n",
    "    \"\"\"\n",
    "    \n",
    "    ### ===> Your code begins here\n",
    "    raise NotImplementedError(\"Function predict_logistic() is not implemented\")\n",
    "    ### ===> Your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"background-color:#abebc6\">2. Testing the training and prediction algorithms on toy datasets</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color:#abebc6\">2.1. 2D data, separable case</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Generate two blobs of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two blobs\n",
    "N = 300\n",
    "X, y = make_blobs(n_samples=N, centers=2, cluster_std=1, n_features=2, random_state=2)\n",
    "\n",
    "# change labels 0 to -1\n",
    "y[y==0] = -1\n",
    "\n",
    "print(\"X.shape =\", X.shape, \"  y.shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Let's plot the blobs of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "# plot negatives in red\n",
    "plt.scatter(X[y==-1,0], \\\n",
    "            X[y==-1,1], \\\n",
    "            alpha = 0.5,\\\n",
    "            c = 'red')\n",
    "\n",
    "# and positives in blue\n",
    "plt.scatter(x=X[y==1,0], \\\n",
    "            y=X[y==1,1], \\\n",
    "            alpha = 0.5, \\\n",
    "            c = 'blue')\n",
    "\n",
    "P=+1\n",
    "N=-1\n",
    "legend_elements = [ Line2D([0], [0], marker='o', color='r',\\\n",
    "                    label='Class %d'%N, markerfacecolor='r',\\\n",
    "                    markersize=10),\\\n",
    "                    Line2D([0], [0], marker='o', color='b',\\\n",
    "                    label='Class %d'%P, markerfacecolor='b',\\\n",
    "                    markersize=10) ]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='best')\n",
    "plt.show()      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"background-color: #f7dc6f\">2.1.3. Let's train the logistic regressor and plot the loss curve</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(567)\n",
    "\n",
    "# ==> Replace the right hand side below with a call to the\n",
    "# train_logistic() function defined above. Use parameter return_history=True\n",
    "\n",
    "w_logistic, loss = np.array([0,0,0]), [0]\n",
    "\n",
    "# ==> Your code insert ends here\n",
    "\n",
    "print()\n",
    "print(\"Final weight:\\n\", w_logistic)\n",
    "print()\n",
    "print(\"Final loss:\\n\", loss[-1])\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Now, let's plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x1min = min(X[:,0])\n",
    "x1max = max(X[:,0])\n",
    "x2min = min(X[:,1])\n",
    "x2max = max(X[:,1])\n",
    "\n",
    "y_pred = predict_logistic(X, w_logistic)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_title(\"Ground-truth\")\n",
    "\n",
    "# plot negatives in red\n",
    "ax1.scatter(X[y==-1,0], \\\n",
    "            X[y==-1,1], \\\n",
    "            alpha = 0.5, \\\n",
    "            c = 'red')\n",
    "\n",
    "# and positives in blue\n",
    "ax1.scatter(x=X[y==1,0], \\\n",
    "            y=X[y==1,1], \\\n",
    "            alpha = 0.5, \\\n",
    "            c = 'blue')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "ax2.set_title(\"Prediction\")\n",
    "ax2.scatter(x = X[:,0], y = X[:,1], c = -y_pred, cmap = 'coolwarm')\n",
    "ax2.legend(handles=legend_elements, loc='best')\n",
    "ax2.set_xlim([x1min-1, x1max+1])\n",
    "ax2.set_ylim([x2min-1, x2max+1])\n",
    "\n",
    "p1 = (x1min, -(w_logistic[0] + (x1min)*w_logistic[1])/w_logistic[2])\n",
    "p2 = (x1max, -(w_logistic[0] + (x1max)*w_logistic[1])/w_logistic[2])\n",
    "\n",
    "lines = ax2.plot([p1[0], p2[0]], [p1[1], p2[1]], '-')\n",
    "plt.setp(lines, color='g', linewidth=4.0)\n",
    "\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"background-color: #f7dc6f\">2.1.5.  Counting errors</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"background-color: #f7dc6f\">2.1.6.  Convergence</font>\n",
    "\n",
    "Try the above training with different number of iterations and learning rates. Comment how did you vary these parameters and what you have observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color:#abebc6\">2.2.  2D data, overlapping blobs</font>\n",
    "\n",
    "<br>\n",
    "<font style=\"background-color: #f7dc6f\">Repeat 2.1, with overlapping blobs</font>\n",
    "\n",
    "1. Generate the two classes of points (with cluster overlap) -- see documentation of function <tt>make_blobs()</tt>\n",
    "2. Plot the points\n",
    "3. Train the classifier and plot the loss curve\n",
    "4. Plot the decicion boundary\n",
    "5. Compute the number of misclassified points\n",
    "\n",
    "You may just copy and paste the cells of Section 2.1 and make appropriate changes\n",
    "\n",
    "(Repeating codes is not recommended, but here we do not need to follow it strictly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color:#abebc6\">2.3. 2D data with overlapping and unbalanced classes</font>\n",
    "\n",
    "<br>\n",
    "<font style=\"background-color: #f7dc6f\">Repeat 2.2 for unbalanced classes</font>\n",
    "\n",
    "Repeat the last case (Section 2.2), but now considering a unbalanced class distribution.\n",
    "You may try 80% x 20%, for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color:#abebc6\">2.4. 3D+ data</font>\n",
    "\n",
    "This section is to make sure your implementation works for data points with dimension $d>2$.\n",
    "\n",
    "<font style=\"background-color: #f7dc6f\">Write code for blobs that are sufficiently separated and with dimension $d$ at least 3. Compute the number of misclassified points at the end.</font>\n",
    "\n",
    "You may try larger dimension and also overlapping blobs.\n",
    "\n",
    "In this sestion, there is no need to plot the blob of points nor the decision boundary (3D plot may be misleading and we are not able to plot data of larger dimension anyway ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
